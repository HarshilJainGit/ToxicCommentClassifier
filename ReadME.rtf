{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red17\green0\blue231;\red29\green38\blue42;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c9689\c9755\c92705;\cssrgb\c14902\c19608\c21961;
}
\margl1440\margr1440\vieww28600\viewh15300\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 The zip contains only the notebook files containing the code and a readme file. All the other supporting files are on \
Github (
\f1 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 https://github.com/gaurav199494/multilabel-comment-classification
\f0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 )\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Steps to run:\
1) Unzip the folder\
2) Make a new folder called \'91data\'92 which contains the dataset (data folder on Github)\
3) You will also have to download the word embedding file from the following URL: 
\fs26 \cf4 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 \'a0(https://fasttext.cc/docs/en/english-vectors.html)\
4) In the unzipped folder containing the Jupiter notebook files, create a new folder called \'91crawl-300d-2M.vec\'92. Place the downloaded word embedding file in this folder. 
\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
5) The main folder will contain 5 Jupyter Notebook files and 1 README file\
6) Open command prompt and navigate to the unzipped folder containing all the content mentioned above.\
7) A new tab would open on the browser for Jupyter Notebook.\
\
\
Logistic Regression: (logistic.ipynb)\
Once you run all the code blocks, finally a results csv file called \'91logistic_results\'92 would be generated. This file will contain the actual predictions for all 6 classes of a comment.\
After this you can run the create_cm file by giving in the correct name to the paths and files. This would generate accuracy and loss values for all 6 classes. And finally it\
 would generate a file called \'91average_logistic\'92 containing the accuracy, balanced accuracy, log loss and hamming loss.\
\
Naive Bayes SVM: (NBSVM.ipynb)\
Once you run all the code blocks, finally a results csv file called \'91NBSVM_results\'92 would be generated. This file will contain the actual predictions for all 6 classes of a comment.\
After this you can run the create_cm file by giving in the correct name to the paths and files. This would generate accuracy and loss values for all 6 classes. And finally it\
 would generate a file called \'91average_NBSVM\'92 containing the accuracy, balanced accuracy, log loss and hamming loss.\
\
CNN: (ConvolutionalNeuralNetwork.ipynb)\
Once you run all the code blocks, finally a results csv file called \'91cnn_results\'92 would be generated. This file will contain the actual predictions for all 6 classes of a comment.\
After this you can run the create_cm file by giving in the correct name to the paths and files. This would generate accuracy and loss values for all 6 classes. And finally it\
 would generate a file called \'91average_cnn\'92 containing the accuracy, balanced accuracy, log loss and hamming loss.\
\
It also has a file for Data Visualization (DataVisualization.ipynb)which contains plots related to the data.\
}